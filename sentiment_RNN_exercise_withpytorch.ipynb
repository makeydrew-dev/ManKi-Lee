
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXLHWo4n6eKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djKNvR07r86M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O6Un8PGr88p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJzekk-yr8-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 단어가 10000개에 달하기 때문에 임베딩 레이어를 생성하여 효과적인 결과를 나타낼 수 있는 input 데이터를 만드는 것이 필요하다\n",
        "# 임베딩 레이어는 감성 학습을 하기위한 용도보다는 차원의 감소를 위해 쓰인다.\n",
        "# "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2YvXv-0uWGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfIFDHySuaEn",
        "colab_type": "text"
      },
      "source": [
        "- 단어가 10000개에 달하기 때문에 임베딩 레이어를 생성하여 효과적인 결과를 나타낼 수 있는 input 데이터를 만드는 것이 필요하다\n",
        "- 임베딩 레이어는 감성 학습을 하기위한 용도보다는 차원의 감소를 위해 쓰인다.\n",
        "- 임베딩 레이어를 통과한 input데이터는 LSTM cell을 지나며 새로운 임베딩 과정을 거친다.\n",
        "- 최종적으로 LSTM 아웃풋이 sigmoid output layer로 향하게 된다\n",
        "- sigmoid function을 활용하여 1(긍정) or 0(부정)으로 나뉘게 되며, 이는 예측되는 output에 해당하는 0과 1 사이의 감성값으로 나오게 된다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1LMPzApu7Wo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "45a39970-f12c-41cc-bf42-d83b4615face"
      },
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31m  ERROR: HTTP error 403 while getting http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n",
            "\u001b[31m  ERROR: Could not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl because of error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n",
            "\u001b[31mERROR: Could not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl because of HTTP error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl for URL http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IU6qflhu9xH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "f150144f-9add-48e3-94f7-028e9b4cebbb"
      },
      "source": [
        "!mkdir data\n",
        "!wget -c https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/labels.txt\n",
        "!wget -c https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/reviews.txt\n",
        "!mv *.txt data/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-29 12:08:35--  https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/labels.txt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/labels.txt [following]\n",
            "--2020-06-29 12:08:36--  https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/labels.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 225000 (220K) [text/plain]\n",
            "Saving to: ‘labels.txt’\n",
            "\n",
            "labels.txt          100%[===================>] 219.73K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-06-29 12:08:37 (4.07 MB/s) - ‘labels.txt’ saved [225000/225000]\n",
            "\n",
            "--2020-06-29 12:08:39--  https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/reviews.txt\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/reviews.txt [following]\n",
            "--2020-06-29 12:08:40--  https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/reviews.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33678267 (32M) [text/plain]\n",
            "Saving to: ‘reviews.txt’\n",
            "\n",
            "reviews.txt         100%[===================>]  32.12M  41.6MB/s    in 0.8s    \n",
            "\n",
            "2020-06-29 12:08:42 (41.6 MB/s) - ‘reviews.txt’ saved [33678267/33678267]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzjyb8wLvKd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# read data from text files\n",
        "with open('data/reviews.txt', 'r') as f:\n",
        "    reviews = f.read()\n",
        "with open('data/labels.txt', 'r') as f:\n",
        "    labels = f.read()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5kykH0BvW02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "ee15ebf1-f0c3-408b-8d8c-0e290484fea9"
      },
      "source": [
        "print(reviews[:2000])\n",
        "print()\n",
        "print(labels[:20])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
            "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
            "homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y\n",
            "\n",
            "positive\n",
            "negative\n",
            "po\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY1b3PcQva-3",
        "colab_type": "text"
      },
      "source": [
        "## data pre-processing\n",
        "- neural network model을 형성할 때 첫번째 단계는 network에 넣기 위한 적절한 형식으로 데이터를 수집하는 것\n",
        "- 적절한 형태를 만들기 위해 단어를 인코드하여 integer형태로 만들어야 한다\n",
        "- 1) 마침표와 불필요한 구두점을 제거\n",
        "- 2) 리뷰는 /n으로 구분된다. 이를 처리하기 위해 /n을 구분 기호로 사용하여 텍스트를 각 리뷰로 분할\n",
        "- 3) 모든 리뷰를 하나의 큰 문자열로 다시 결합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvD051jFwQFn",
        "colab_type": "text"
      },
      "source": [
        "### 모든 구두점을 제거. 줄바꿈 없이 모든 텍스트로 가져와서 개별 단어로 바꿈"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSmq08OOvX0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be790ed9-cbac-4157-c0a2-435d40f2396f"
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "print(punctuation)\n",
        "\n",
        "# get rid of punctuation\n",
        "reviews = reviews.lower() # lowercase, standardize\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U92HuqHwg7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split by new lines and spaces\n",
        "reviews_split = all_text.split('\\n')\n",
        "all_text = ' '.join(reviews_split)\n",
        "\n",
        "# create a list of words\n",
        "words = all_text.split()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM4rsXRjwjo1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "1436609d-2a0a-4efd-fe23-102526c8b5f9"
      },
      "source": [
        "words[:30]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell',\n",
              " 'high',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cartoon',\n",
              " 'comedy',\n",
              " 'it',\n",
              " 'ran',\n",
              " 'at',\n",
              " 'the',\n",
              " 'same',\n",
              " 'time',\n",
              " 'as',\n",
              " 'some',\n",
              " 'other',\n",
              " 'programs',\n",
              " 'about',\n",
              " 'school',\n",
              " 'life',\n",
              " 'such',\n",
              " 'as',\n",
              " 'teachers',\n",
              " 'my',\n",
              " 'years',\n",
              " 'in',\n",
              " 'the',\n",
              " 'teaching',\n",
              " 'profession',\n",
              " 'lead',\n",
              " 'me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQB-EdK0wmx2",
        "colab_type": "text"
      },
      "source": [
        "## encoding the words\n",
        "- encoding을 위해 network에 integer 값들을 넣어야 함\n",
        "- 가장 쉬운 방법은 딕셔너리를 만들어서 문자와 숫자를 매핑 하는 것\n",
        "- 이렇게 함으로써 리뷰들을 integer로 바꿀 수 있게 됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrnRTUsbwkoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feel free to use this import \n",
        "from collections import Counter\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab,1)} \n",
        "\n",
        "## use the dict to tokenize each review in reviews_split\n",
        "## store the tokenized reviews in reviews_ints\n",
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "  reviews_ints.append([vocab_to_int[word] for word in review.split()])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTMUTOxIxX5N",
        "colab_type": "text"
      },
      "source": [
        "## 코드 테스트 해보기\n",
        "- 딕셔너리의 정확성을 확보하기 위해\n",
        "- 사전을 정확하게 구현 한 텍스트로 어휘의 고유 단어 수와 토큰 화 된 첫 번째 리뷰의 내용을 인쇄"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjV1VLFpxKer",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "d57d2f5f-507d-4cd0-a73e-b76e9b6d5128"
      },
      "source": [
        "# stats about vocabulary\n",
        "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
        "print()\n",
        "\n",
        "# print tokens in first review\n",
        "print('Tokenized review: \\n', reviews_ints[:1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words:  74072\n",
            "\n",
            "Tokenized review: \n",
            " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0uj9TL-x7mN",
        "colab_type": "text"
      },
      "source": [
        "## encoding the labels\n",
        "- label 은 긍정과 부정으로 구성됨\n",
        "- 따라서 네트워크에 있는 label을 0과 1로 바꿔야 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_xCH6KTxWi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1=positive, 0=negative label conversion\n",
        "labels_split = labels.split('\\n')\n",
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-PiQ7lEyNid",
        "colab_type": "text"
      },
      "source": [
        "## removing outliers\n",
        "- 추가적인 전처리 과정에서, standard processing에 맞는 모양을 형성하고 있는지 확실히 살펴야 한다.\n",
        "- 따라서 특정한 길이에 맞게 우리의 리뷰 모양을 바꿔야 한다\n",
        "1) 지나치게 길거나 짧은 리뷰들을 삭제\n",
        "2) 같은 리뷰로 맞춰주기 위해 부족한 데이터에는 패딩을 더하고 과한 것은 줄여준다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfyle4xtyIWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "64b520a2-0ce3-4e89-845e-08ad8e4b9151"
      },
      "source": [
        "# outlier review stats\n",
        "review_lens = Counter([len(x) for x in reviews_ints])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 1\n",
            "Maximum review length: 2514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juXLPrb8zIEd",
        "colab_type": "text"
      },
      "source": [
        "- 봤더니, 길이가 0인 리뷰 하나와 지나치게 긴 리뷰가 발견됨\n",
        "- 요놈들을 제거한다면 훨씬 효과적인 학습 결과를 얻을 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Qy59ZfzG8W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c22a5126-8749-4fec-8826-d6f4a660ef33"
      },
      "source": [
        "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
        "\n",
        "## remove any reviews/labels with zero length from the reviews_ints list.\n",
        "\n",
        "## get any indices of any reviews with length 0\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
        "\n",
        "# remove 0-length review with their labels\n",
        "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_ints))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before removing outliers:  25001\n",
            "Number of reviews after removing outliers:  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDBnIHtbzeqs",
        "colab_type": "text"
      },
      "source": [
        "## padding sequences\n",
        "- 짧은 것은 더해주고 긴 것은 줄여주는 방식으로 진행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H85E_sc7zchF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(reviews_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's \n",
        "        or truncated to the input seq_length.\n",
        "    '''\n",
        "    ## getting the correct rows x cols shape\n",
        "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "    \n",
        "    ## for each review, I grab that review\n",
        "    for i, row in enumerate(reviews_ints):\n",
        "      features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ4QhjH3z4jk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "374ea2f2-0bdd-41ce-d009-00d6ebb3247c"
      },
      "source": [
        "# Test your implementation!\n",
        "\n",
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "## test statements - do not change - ##\n",
        "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "# print first 10 values of the first 30 batches \n",
        "print(features[:30,:10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
            " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   54    10    14   116    60   798   552    71   364     5]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   330   578    34     3   162   748  2731     9   325]\n",
            " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
            " [    1    20     6    76    40     6    58    81    95     5]\n",
            " [   54    10    84   329 26230 46427    63    10    14   614]\n",
            " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   40    26   109 17952  1422     9     1   327     4   125]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   10   499     1   307 10399    55    74     8    13    30]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lutj1PGU18E9",
        "colab_type": "text"
      },
      "source": [
        "## Training, Validation, Test\n",
        "- 데이터의 모양을 완성하면, 분할을 진행하라!\n",
        "1) 종속변수와 독립변수를 위한 세트로 나눈다, 2) 분리할때 split_frac은 트레이닝 셋을 유지하기 위한 데이터로 정의한다. 주 비율은 0.8 또는 0.9\n",
        "- 남은 데이터는 검증과 테스트를 위한 데이터로 사용하기 위해 절반으로 나눠야 한다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQhPZZHA20Wa",
        "colab_type": "text"
      },
      "source": [
        "## check your work\n",
        "- 각 비율은 0.8, 0.1, 0.1로 나눈다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hykN_Icxz7ZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "b15c9404-9e6c-4259-a41b-184d717dc7a0"
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "split_idx = int(len(features)*0.8)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeatures Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeatures Shapes:\n",
            "Train set: \t\t(20000, 200) \n",
            "Validation set: \t(2500, 200) \n",
            "Test set: \t\t(2500, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG8CrEwY2-V6",
        "colab_type": "text"
      },
      "source": [
        "## Data Loaders and Batching\n",
        "- 데이터를 나눈 후에 dataloader를 생성하기 위해 2단계를 거친다\n",
        "- 1) 데이터를 평가하기 위한 포맷인 TensorDataset을 생성하여 입력 데이터 셋과 동일한 첫번째 차원의 데이터 셋을 가져온다\n",
        "- 2) Dataloaders와 트레이팅,테스트, 검증을 위한 batch를 생성한다.\n",
        "- 이러한 것은 full batch에서 데이터를 batching 하기 위한 generator function을 대체하기 위해 생성할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dZ_eanF2zGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-flvWb24iIL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "cebbb22a-6d06-4ade-bc8f-d145324c50e0"
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,   380,    11,    30],\n",
            "        [    9,     3, 23041,  ...,    80, 26383,  4061],\n",
            "        [    0,     0,     0,  ...,    22,     1,   922],\n",
            "        ...,\n",
            "        [  251,    36,    47,  ...,   276,    92,  2701],\n",
            "        [    0,     0,     0,  ...,   683,  8331,    78],\n",
            "        [   63,    44,   674,  ...,   114,    66,   173]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
            "        1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8FpNhmO6tuw",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Nerwork with PyTorch\n",
        "- 1) 임베딩 레이어에서는 단어 토큰들을 임베딩에 적합한 적정 사이즈로 변환해준다\n",
        "- 2) LSTM 레이어는 hidden state size와 layer의 수를 정의한다\n",
        "- 3) fully connected output layer는 LSTM레이어를 매핑하여 원하는 output_size를 도출해낸다\n",
        "- 4) sigmoid activation layer는 모든 output 값들을 0-1 사이로 나오게 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ztqnnjp6v7A",
        "colab_type": "text"
      },
      "source": [
        "## The Embedding Layer\n",
        "- 74000+ 개의 단어를 임베딩 진행함\n",
        "- 거대한 데이터를 one-hot encoding 해서 진행하는 것은 비용적으로 비효율적\n",
        "- one-hot encoding 대신 임베딩 레이어를 사용, word2vec을 사용해 embedding 을 진행함\n",
        "- 차원 축소를 위해서만 임베딩을 사용하고, 네트워크가 가중치를 배우도록 하는 것이 좋음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xljnO7Y49X8A",
        "colab_type": "text"
      },
      "source": [
        "## LSTM layer\n",
        "- LSTM레이어를 만들 것인데, input_size와 hidden_dim, layer의 수, dropout 확률, batch_first 의 파라미터를 설정해야 한다.\n",
        "- 대부분의 경우 2-3개 사이의 layer를 쌓는 것이 포포먼쓰에 좋다. 지나친 레이어의 추가는 연산을 너무 복잡하게 만들 수 있다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c3AAOa_6ssQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6fb60395-97eb-4d3a-d12d-5cda4da8380c"
      },
      "source": [
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKTAJEhg-Gzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leIDeRQXBFN0",
        "colab_type": "text"
      },
      "source": [
        "## Instantiate the network\n",
        "- 네트워크를 인스턴트화(?)하기 위해 하이퍼파라미터를 정의한다\n",
        "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
        "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
        "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
        "* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
        "* `n_layers`: Number of LSTM layers in the network. Typically between 1-3\n",
        "\n",
        "> **Exercise:** Define the model  hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWyxLjvE_D3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "1e880de2-ecd1-45dc-8611-42bf07592642"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int) + 1 # +1 for zero padding + our word tokens\n",
        "output_size = 1 # 긍정 혹은 부정이기 때문에 '1'로 한듯\n",
        "embedding_dim = 400 \n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(74073, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCCdpbpzDk4V",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "- sigmoid으로 인해 나오는 output를 설계하기 위해 새로운 종류의 cross entropy loss를 사용 (BCELoss)\n",
        "- 훈련 hyperparameter를 설정해야 함\n",
        "\n",
        "* 'lr' : optimizer의 learninig rate\n",
        "* 'epochs' : epochs의 수를 성정\n",
        "* 'clip' : gradient 의 최대값을 설정해 둠"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQq_-WX3_FrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVPhuqb6_H8V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "c017496b-bbbe-4c6f-b6b3-b1396999a7a7"
      },
      "source": [
        "# training params\n",
        "\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.659967... Val Loss: 0.635598\n",
            "Epoch: 1/4... Step: 200... Loss: 0.600294... Val Loss: 0.605430\n",
            "Epoch: 1/4... Step: 300... Loss: 0.543149... Val Loss: 0.590107\n",
            "Epoch: 1/4... Step: 400... Loss: 0.678727... Val Loss: 0.632917\n",
            "Epoch: 2/4... Step: 500... Loss: 0.526109... Val Loss: 0.543538\n",
            "Epoch: 2/4... Step: 600... Loss: 0.381847... Val Loss: 0.516834\n",
            "Epoch: 2/4... Step: 700... Loss: 0.278988... Val Loss: 0.464112\n",
            "Epoch: 2/4... Step: 800... Loss: 0.260972... Val Loss: 0.470605\n",
            "Epoch: 3/4... Step: 900... Loss: 0.182339... Val Loss: 0.532921\n",
            "Epoch: 3/4... Step: 1000... Loss: 0.305787... Val Loss: 0.436250\n",
            "Epoch: 3/4... Step: 1100... Loss: 0.249118... Val Loss: 0.444607\n",
            "Epoch: 3/4... Step: 1200... Loss: 0.490452... Val Loss: 0.714605\n",
            "Epoch: 4/4... Step: 1300... Loss: 0.091725... Val Loss: 0.474582\n",
            "Epoch: 4/4... Step: 1400... Loss: 0.034581... Val Loss: 0.504221\n",
            "Epoch: 4/4... Step: 1500... Loss: 0.148437... Val Loss: 0.470923\n",
            "Epoch: 4/4... Step: 1600... Loss: 0.204735... Val Loss: 0.477349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0jVtoqEEiVQ",
        "colab_type": "text"
      },
      "source": [
        "## Testing\n",
        "- test data performance : 1) 트레이닝 한 모델이 test 셋을 상대로 어느정도의 성능을 보이는지 확인, 2) loss의 평균값과 정확도를 계산\n",
        "- 훈련된 모델이 예측하는 것을 살펴봄"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjIPIECY_JH9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8a92ad50-f952-4d94-992b-0976eaee5564"
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.472\n",
            "Test accuracy: 0.788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSxoRxTtFQH3",
        "colab_type": "text"
      },
      "source": [
        "## inference on a test review\n",
        "- 내가 원하는 어떤 텍스트던지 test에 넣어서 진행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhLRtnDaAtd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# negative test review\n",
        "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYShe5R8GeUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueSh1vwXAwH8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ab95aa3-01ae-4fc0-ed75-07cbb78e8a22"
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "def tokenize_review(test_review):\n",
        "    test_review = test_review.lower() # lowercase\n",
        "    # get rid of punctuatuon\n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "    \n",
        "    # splitting by spaces\n",
        "    test_words = test_text.split()\n",
        "    \n",
        "    # tokens\n",
        "    test_ints = []\n",
        "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
        "    \n",
        "    return test_ints\n",
        "  \n",
        "# test code and generate tokenized review\n",
        "test_ints = tokenize_review(test_review_neg)\n",
        "print(test_ints)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 247, 18, 10, 28, 108, 113, 14, 388, 2, 10, 181, 60, 273, 144, 11, 18, 68, 76, 113, 2, 1, 410, 14, 539]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD-92uKFAxCE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "41735ba4-5cea-4c0f-aa24-de29fdef749d"
      },
      "source": [
        "# test sequence padding\n",
        "seq_length = 200\n",
        "features = pad_features(test_ints, seq_length)\n",
        "\n",
        "print(features)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   1 247  18  10  28\n",
            "  108 113  14 388   2  10 181  60 273 144  11  18  68  76 113   2   1 410\n",
            "   14 539]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8jrtJTEAyxc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3257d271-b96e-4bd3-be88-c83b9be795c8"
      },
      "source": [
        "# test conversion to tensor and pass it to model\n",
        "feature_tensor = torch.from_numpy(features)\n",
        "print(feature_tensor.size())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIhEaz2BAz80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, test_review, sequence_length=200):\n",
        "    ''' Prints out whether a give review is predicted to be \n",
        "        positive or negative in sentiment, using a trained model.\n",
        "        \n",
        "        params:\n",
        "        net - A trained net \n",
        "        test_review - a review made of normal text and punctuation\n",
        "        sequence_length - the padded length of a review\n",
        "        '''\n",
        "    \n",
        "    net.eval()\n",
        "    \n",
        "    # tokenize review\n",
        "    test_ints = tokenize_review(test_review)\n",
        "    \n",
        "    # pad tokenize sequence\n",
        "    seq_length = sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "    \n",
        "    # convert to tensor to pass to model\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "      feature_tensor = feature_tensor.cuda()\n",
        "      \n",
        "    # get the output from the model\n",
        "    output, h = net(feature_tensor, h)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())\n",
        "    # printing output value, before rounding\n",
        "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "    \n",
        "    # print custom response based on whether test_review is pos/neg\n",
        "    if(pred.item()==1):\n",
        "      print('Positive review detected!')\n",
        "    else:\n",
        "      print('Negative review detected!')\n",
        "    \n",
        "        "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y813ok9iA0y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# positive test review\n",
        "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiEd7zzmA2mE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "65b89a3f-824a-4288-bc33-c308b4c9e0ee"
      },
      "source": [
        "# call function\n",
        "# try negative and positive reviews!\n",
        "seq_length=200\n",
        "predict(net, test_review_neg, seq_length)\n",
        "predict(net, test_review_pos, seq_length)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction value, pre-rounding: 0.007140\n",
            "Negative review detected!\n",
            "Prediction value, pre-rounding: 0.904111\n",
            "Positive review detected!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6_5_RlQA4Bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
